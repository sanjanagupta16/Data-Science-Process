---
title: "Airbnb"
author: "Sanjana Gupta"
date: "30/01/2020"
output:
  html_document:
    df_print: paged
  pdf_document: default
always_allow_html: yes
---

#### GitHub Repo - https://github.com/sanjanagupta16/Data-Science-Process

Importing Libraries and setting cwd
```{r libraries, warning=FALSE, errorFALSE}
library(tidyverse)
library(plotly)
library(ggplot2)
library(naniar)
library(leaflet)
library(htmltools)
library(choroplethr)
library(dplyr)
library(choroplethrMaps)
library(ggplot2)
library(gridExtra)
library(ggmap)
library(htmlwidgets)
library(mapview)
library(dplyr)
library(plotly)
library(devtools)
library(kableExtra)
library(here)

#setwd("~/Desktop/Winter 2020/Data 598C Data Science Process/Data Science Project")
#setwd(here("/Data Science Project"))
```

Importing data and creating a training dataset
```{r import-data}

data1 <- read.csv(file = "data/listings.csv", header = TRUE)
data2 <- read.csv(file = "data/calendar.csv")
data3 <- read.csv(file = "data/reviews.csv")

data_train <- data1
```

Viewing the data in different formats
```{r view-data}
#View(data_train)

#to view data kinda nicely - uses dplyr
#glimpse(data_train)
#str(data1)
#summary(data_train)
#head(data_train, 1000)
#tail(data_train, 50)
```

## Data Understanding & Data Cleanng

1. Data Cleaning - removing columns
```{r data-clean}
#cols_to_delete <- c("last_scraped", "thumbnail_url", "host_picture_url", "medium_url", "picture_url", "xl_picture_url", "host_thumbnail_url", "host_picture_url")
#select(data_train, -contains("url"))
data_train$last_scraped <- NULL
data_train$thumbnail_url <- NULL
data_train$host_picture_url <- NULL
data_train$medium_url <- NULL
data_train$picture_url <- NULL
data_train$xl_picture_url <- NULL
data_train$host_thumbnail_url <- NULL
data_train$host_picture_url <- NULL
data_train$host_url <- NULL
data_train$scrape_id <- NULL
data_train$experiences_offered <- NULL
data_train$neighborhood_overview <- NULL
data_train$host_about <- NULL
data_train$host_id <- NULL
data_train$host_verifications <- NULL
data_train$host_has_profile_pic <- NULL
data_train$host_identity_verified <- NULL
data_train$calendar_last_scraped <- NULL
data_train$license <- NULL
data_train$require_guest_profile_picture <- NULL
data_train$require_guest_phone_verification <- NULL
data_train$summary <- NULL
data_train$notes <- NULL
data_train$space <- NULL
data_train$description <- NULL
data_train$transit <- NULL
data_train$host_neighbourhood <- NULL
data_train$market <- NULL
data_train$country_code <- NULL
data_train$is_location_exact <- NULL

rownames(data_train) <- NULL

#Viewing the data
#glimpse(data_train)
#summary(data_train$is_location_exact)
```

2. Converting Data type for price, cleaning fee, extra_people, security deposit to numeric
```{r convert-num}
data_train$price <- sub("\\$","",data_train$price)
data_train$price <- sub(",","",data_train$price)
data_train$price <- as.integer(data_train$price)

data_train$cleaning_fee <- sub("\\$","",data_train$cleaning_fee)
data_train$cleaning_fee <- sub(",","",data_train$cleaning_fee)
data_train$cleaning_fee <- as.integer(data_train$cleaning_fee)

data_train$extra_people <- sub("\\$","",data_train$extra_people)
data_train$extra_people <- sub(",","",data_train$extra_people)
data_train$extra_people <- as.integer(data_train$extra_people)

data_train$security_deposit <- sub("\\$","",data_train$security_deposit)
data_train$security_deposit <- sub(",","",data_train$security_deposit)
data_train$security_deposit <- as.integer(data_train$security_deposit)
```

3. Changing data type of categorical variables to factor
```{r convert-factor}
data_train$host_response_time <- as.factor(data_train$host_response_time)
data_train$host_is_superhost <- as.factor(data_train$host_is_superhost)
data_train$neighbourhood_cleansed <- as.factor(data_train$neighbourhood_cleansed)
data_train$property_type <- as.factor(data_train$property_type)
data_train$room_type <- as.factor(data_train$room_type)
data_train$bed_type <- as.factor(data_train$bed_type)
data_train$calendar_updated <- as.factor(data_train$calendar_updated)
data_train$cancellation_policy <- as.factor(data_train$cancellation_policy)
data_train$instant_bookable <- as.factor(data_train$instant_bookable)
data_train$bed_type <- as.factor(data_train$bed_type)
```

4. Changing host reponse time, acceptance rate from factor to numeric
```{r convert-num2}
data_train$host_response_rate<- as.numeric(gsub("%", "", as.character(data_train$host_response_rate)))/100
data_train$host_acceptance_rate <- as.numeric(gsub("%", "", as.character(data_train$host_acceptance_rate)))/100
```

5. For Property Type: Replacing the categories with only a few listings to "Other" 
```{r}
#summary(data_train$property_type)
#data_train$property_type <- replace.value(data_train$property_type, c("Boat", "Bungalow", "Cabin", "Tent","Treehouse", "Yurt"),"Other")
data_train$property_type[data_train$property_type == "Boat"] <- "Other"
data_train$property_type[data_train$property_type == "Bungalow"] <- "Other"
data_train$property_type[data_train$property_type == "Dorm"] <- "Other"
data_train$property_type[data_train$property_type == "Chalet"] <- "Other"
data_train$property_type[data_train$property_type == "Treehouse"] <- "Other"
data_train$property_type[data_train$property_type == "Yurt"] <- "Other"
data_train$property_type[data_train$property_type == "Camper/RV"] <- "Other"

summary(data_train$property_type)
```


6. Checking if Price = 0 for any listings
```{r check-price}
0 %in% data_train$price
```
All Airbnb listings have a price associated with it

7. Removing rows with NA
```{r remove-NA}
#is.null(data_train$zipcode)
data_train %>% filter_all(all_vars(!is.na(.)))
data_train %>% filter_all(all_vars(complete.cases(.)))  
```
8. Removing rows with null values
```{r remove-null}
data_train %>% filter_all(all_vars(!is.null(.)))
data_train %>% filter_all(all_vars(complete.cases(.)))  
```

9. Getting a summary of all fields
```{r summary}
summary_df <- data.frame(summary(data_train))
#write.csv(summary_df, "summary.csv")
```

10. Adding log price column and log accommodates column --- Transformations
```{r log-Price}
data_train$log_price <- log(data_train$price)
data_train$log_acc <- log(data_train$accommodates)
```


11. Checking for missing values and plotting the data
```{r missing-data}
check_missing<- function(x){
  if (is.character(x)) sum(x=="") else sum(is.na(x))
}
NMISS<-data.frame(nmiss=sapply(data_train,check_missing))
#write.csv(NMISS, "missing.csv")

#plotting missing data
missingdata <- data_train
missingdata[missingdata == ""] <- NA
missingdata <- missingdata %>% select(host_is_superhost, review_scores_rating,
                                      host_response_time, name, zipcode, latitude,longitude,
                                      host_location, host_response_rate,neighbourhood_cleansed,
                                      property_type, price,weekly_price, monthly_price, bedrooms, bathrooms)
gg_miss_var(missingdata) + labs(title = "Missing Values Plot",y = "Missing Data", x = "Attribute")  + theme_bw() + theme(text=element_text(color = "dark blue")) 
```

12. Replacing missing values
```{r replace-missing}
data_train$review_scores_rating <- ifelse(is.na(data_train$review_scores_rating)==T,97,data_train$review_scores_rating)
data_train$host_response_rate <- ifelse(is.na(data_train$host_response_rate)==T,97,data_train$host_response_rate)

#Using same plot as before to check the values
gg_miss_var(missingdata) + labs(title = "Missing Values Plot",y = "Missing Data", x = "Attribute")  + theme_bw() + theme(text=element_text(color = "dark blue")) 
```

## Data Understanding - plots

1. Growth of Airbnb - evolution of hosts over time
```{r}
#Removing NA 
new_hosts <- data_train %>% drop_na("host_since")
#str(new_hosts$host_since)

# Calculate the number of new hosts for each year 
new_hosts$host_since <- as.Date(new_hosts$host_since, '%Y-%m-%d')
new_hosts <- new_hosts[new_hosts$host_since < as.Date("2018-01-01"),]
new_hosts <- new_hosts[order(as.Date(new_hosts$host_since, format="%Y-%m-%d")),]
new_hosts$host_since <- format(as.Date(new_hosts$host_since, "%Y-%m-%d"), format="%Y-%m")
new_hosts_table <- table(new_hosts$host_since)

# Plot
plot(as.Date(paste(format(names(new_hosts_table), format="%Y-%m"),"-01", sep="")), as.vector(new_hosts_table), type = "l", xlab = "Time", ylab = "Number of new hosts", col = "Blue") 
axis(1, seq(0,2018, 100))
```
The number of new hosts have been steadily increasing since 2008, however, in the last couple of years there has been a decrease in this number.

2. Analysing neighborhood grouping
```{r neighborhood}
#In neighborhood group cleansed, there are 794 rows with "Other Neighborhoods"
#Need to fix this - maybe look at neighorhood cleansed column instead?
#Looking at categorical data now
sorted <- sort(table(data_train$host_neighbourhood), decreasing = TRUE)
#head(subset(data_train, select = 'neighbourhood_group_cleansed'))
fact <- factor(data_train$neighbourhood_group_cleansed)
count(data_train, 'neighbourhood_group_cleansed')
count(data_train, 'neighbourhood_cleansed')

#Getting a count of listings per neighborhood
#write.csv(neigborhood_data, "neighborhoodcount.csv")
#count(data_train$neighbourhood_cleansed)
plot(data_train$neighbourhood_cleansed, xlab = 'Neighborhoods', ylab = 'Count')
neighborhood <- (data_train$neighbourhood_cleansed)
#x <- sort(count((neighborhood)))
#arrange(data_train, neighbourhood_cleansed)
neighborhood_data <- data_train[order(data_train[,21] ),]
#summary(neighborhood_data$neighbourhood_cleansed)
```

```{r plot1}
#Trying to fix "Other Neighborhoods" value in neighborhood group cleaned column
#for(i in data_train)
 # if(i[neighbourhood_group_cleansed] == "Other neighborhoods")
  #  i[neighbourhood_group_cleansed] <-  i[neighborhood_cleansed]

#Plotting count of listings per neighborhood
neigborhood_data <- table(data_train$neighbourhood_cleansed)

barplot(sort(table(data_train$neighbourhood_group_cleansed), decreasing = TRUE), ylab = "Count",horiz = FALSE , main = "Distribution of Listings Per Neighborhood", beside = FALSE)
```

```{r plot2}
#barplot(table(data_train$neighbourhood_group_cleansed), density = 20, ylab = "Count",horiz = FALSE , main = "Distribution of Listings Per Neighborhood", beside = FALSE) + theme(axis.text.x = element_text(angle = 90, hjust = 1))
ggplot(data_train, aes(neighbourhood_group_cleansed)) + geom_bar(na.rm = TRUE, stat = "count") + theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

3. Looking at some more plots - Count of Host Listings
```{r count-listings}
#barplot(data_train$host_listings_count, main = "Count of Host Listings", xlab = "", ylab = "Count") 
ggplot(data_train, aes(host_listings_count)) + geom_bar(na.rm = TRUE) + xlim(0,21) 
```
# Most hosts have a single Airbnb listing.

4. Looking at Types of Airbnb properties
```{r property_type}
#barchart(data_train$property_type, main = "Types of Airbnb Properties", xlab = "Count")
prop_type <- table(data_train$property_type)
barplot(table(data_train$property_type), ylab = "Count",horiz = FALSE, main = "Types of Airbnb Properties", beside = FALSE, las = 1)

ggplot(data_train, aes(property_type)) + geom_bar(na.rm = TRUE, stat = "count") + theme(axis.text.x = element_text(angle = 90, hjust = 1))
#plot(data_train$host_is_superhost)
```

5. Prices per Neighborhood
```{r price-neighborhood}
z = table(data_train$room_type,data_train$neighbourhood_group)
kable(z)  %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed","responsive"))
```


```{r boxplot1}
data_train %>% boxplot(price ~ neighbourhood_group_cleansed,data = ., main="Box Plot of price vs neighbourhood", 
                     ylab="neighbourhood", xlab="Price",horizontal=TRUE)
```

6. Number of listings per neighborhood
```{r listings}
ggplot(data_train, aes(x = fct_infreq(neighbourhood_cleansed), fill = room_type)) +
    geom_bar() +
    labs(title = "No. of listings by neighborhood",
         x = "Neighborhood", y = "No. of listings") +
    theme(legend.position = "bottom") + theme(axis.text.x = element_text(angle = 90, hjust = 1))
```
Looks messy, want to consider looking at only the top 10 neighborhoods

7. Top 10 neighborhoods - doesn't work - need to fix this
```{r top10}
#top_10 <- data_train %>% group_by(neighborhood_cleansed) %>% tally()
#top_10 <- data_train %>% top_n(n = 10, wt = neighbourhood_group_cleansed) %>% arrange(neighbourhood_group_cleansed) %>% #summarize(neighbourhood_group_cleansed = n())

top_10 <- data_train[data_train$neighbourhood_group_cleansed %in% names(sort(table(data_train$neighbourhood_group_cleansed), decreasing = TRUE)[0:1]),]
#top_10 <- data_train %>% top_n(10) %>% group_by(neighbourhood_group_cleansed) %>% tally() 
top_10.val <- table(top_10$neighbourhood_group_cleansed)
kable(top_10.val)
```

7. Looking at outliers for price
```{r outliers}
ggplot(data=data_train, aes(price)) + 
  geom_histogram(fill="red") + 
  labs(title="Histogram of Price") +
  labs(x="Price", y="Count")

# Percentile of price
quantile(data_train$price, c(.9, .95, .97, 0.975, 0.98, 0.99, 0.995, 0.999, 0.9999))
boxplot(data_train$price)
```
8. Room Type Analysis
# For a private room
```{r private-room}
Private_rooms <- data_train %>% filter(room_type == "Private room")
Private_rooms$price %>% summary()
```

# For an entire apartment

```{r entire-apt}
entire_house <- data_train %>% filter(room_type == "Entire home/apt")
entire_house$price %>% summary()
```

#Shared room
```{r shared-room}
shared_room <- data_train %>% filter(room_type == "Shared room")
shared_room$price %>% summary()
```

9. Average prices for each room type
```{r avg-price}
listingdf <- read.csv('data/listings.csv')
zipReviews <- listingdf %>% group_by(zipcode = zipcode) %>% summarise(avg_loc_review = mean(review_scores_location, na.rm = TRUE))

average_price <- data_train %>% group_by(room_type) %>% summarise(price = mean(price))
colnames(average_price)[1] <- "Room Type"
colnames(average_price)[2] <- "Average Price"
print((average_price))
```

10. Top 50 most expensive listings
```{r patial Data 1.3, echo=FALSE, message=FALSE, warning=FALSE}
top_df <- data_train %>% top_n(n = 50, wt = log_price)

# get background map
top_height <- max(top_df$latitude) - min(top_df$latitude)
top_width <- max(top_df$longitude) - min(top_df$longitude)
top_borders <- c(bottom  = min(top_df$latitude)  - 0.1 * top_height,
                 top     = max(top_df$latitude)  + 0.1 * top_height,
                 left    = min(top_df$longitude) - 0.1 * top_width,
                 right   = max(top_df$longitude) + 0.1 * top_width)

top_map <- get_stamenmap(top_borders, zoom = 12, maptype = "toner-lite")

# map of top 50 most expensive listings
ggmap(top_map) +
    geom_point(data = top_df, mapping = aes(x = longitude, y = latitude,
                                        col = price)) +
    scale_color_gradient(low = "blue", high = "red")
```

## Model Selection

#### Clustering
Used Clustering to compare which areas of Seattle have a higher concentration of airbnb listings.

Plotting the listings on the map of Seattle
```{r clustering}
#Reading Listings Data
listingdf <- read.csv('data/listings.csv')
#Creating a link for each pop up

#Creating Listings across Seattle
map <- leaflet(listingdf) %>%
  addTiles() %>%
  addMarkers(~longitude, ~latitude,labelOptions = labelOptions(noHide = T),
             clusterOptions = markerClusterOptions(),
             popup = paste0("<b> Name: </b>", listingdf$name , "<br/><b> Host Name: </b>", 
                            listingdf$host_name, "<br> <b> Price/night: </b>", listingdf$price, "<br/><b> Room Type: </b>", 
                            listingdf$room_type, "<br/><b> Property Type: </b>", listingdf$property_type
             )) %>% 
  setView(-122.335167, 47.608013, zoom = 11) %>%
  addProviderTiles("CartoDB.Positron")
map
mapshot(map, url = paste0(getwd(),"/map.html"))
```
This shows listings in a clustered fashion. Clicking on a cluster zooms into the listings belonging to it.
Clicking on a pin gives you a pop up with some details about the property.
We can see that most of the listings are clustered around Downtown and Cap Hill.

#### Linear Regression

1. Which variables influence price?
Predictor variables selected - bathrooms, price, property type, accomodates, bedrooms, room type, host is superhost, cancellation policy, availability_365, cleaning_fee, extra_people, security_deposit, instant_bookable, maximum_nights, minimum_nights, bed_type, latitude, longitude, number_of_reviews
```{r LR}
LM_model1 <- summary(lm(formula = (price) ~ room_type + accommodates + bedrooms + bathrooms + property_type + host_is_superhost + cancellation_policy + availability_365 +cleaning_fee + extra_people + security_deposit + instant_bookable + maximum_nights + minimum_nights + bed_type + latitude + longitude + number_of_reviews, data = data_train))

plot(price ~room_type + accommodates + bedrooms + bathrooms + property_type + factor(neighbourhood_cleansed) + host_is_superhost, data = data_train) + abline(lm(formula = price ~ room_type + accommodates + bedrooms + bathrooms, data = data_train))
LM_model1
#qqnorm(LM_model1$residuals, main = "Normal qq plot of residuals")

```
Results:

The following features are significant to price:
- Private Room 
- Shared Room
- Accommodates
- Bedrooms
- Bathrooms
- Host is Superhost
- Cleaning Fee
- Security Deposit
- Instant bookable
- Location (Lat/Long)
- Number of reviews

R-squared is 61.99%, which is not bad

2. Taking log(price) with multiple linear regression:
```{r lR-with-logprice}
LM_model2 <- summary(lm(formula = log_price ~ room_type + accommodates + bedrooms + bathrooms + property_type + host_is_superhost +cancellation_policy + availability_365 + cleaning_fee + extra_people + security_deposit + instant_bookable + maximum_nights + minimum_nights + bed_type + latitude + longitude + number_of_reviews, data = data_train))

plot(price ~room_type + accommodates + bedrooms + bathrooms + factor(neighbourhood_cleansed) + host_is_superhost + cancellation_policy + availability_365, data = data_train) + abline(lm(formula = log_price ~ room_type + accommodates + bedrooms + bathrooms, data = data_train))
LM_model2
```
Results:
With transforming price on the log scale, the following features are found to be significant:
- Private Room Type
- Shared Room Type
- Accommodates
- Bedrooms
- Bathrooms
- Host is Superhost
- Availability 365
- Cleaning fee
- Security Deposit
- Instant Bookable
- Minimum Nights
- Latitude
- Longitude
- Number of Reviews

The R-squared metric is 68.68%.

3. Taking log for accomodates: log-accommodates
```{r log-acc}
LM_model3 <- summary(lm(formula = log_price ~ room_type + log_acc + bedrooms + bathrooms + property_type + host_is_superhost + cleaning_fee + cancellation_policy + extra_people + security_deposit + instant_bookable + maximum_nights + minimum_nights + bed_type + latitude + longitude + number_of_reviews + availability_365, data = data_train))

plot(price ~room_type + log_acc + bedrooms + bathrooms + property_type + availability_365 + host_is_superhost + cancellation_policy + cleaning_fee + extra_people, data = data_train) + abline(lm(formula = log_price ~ room_type + log_acc + bedrooms + bathrooms, data = data_train))
LM_model3
```
Results:
With transforming price on the log scale, the following features are found to be significant (same as previous model):
- Private Room Type
- Shared Room Type
- Accommodates
- Bedrooms
- Bathrooms
- Host is Superhost
- Availability 365
- Cleaning fee
- Security Deposit
- Instant Bookable
- Minimum Nights
- Latitude
- Longitude
- Number of Reviews

The R-squared metric is 68.96%.
Since the R-squared value is slightly better with a log transformed accommodates feature, I will be using this going forward.

4. Pairwise comparison
```{r pairwise}
# price vs bathrooms
p1 <- ggplot(data_train, aes(x=bathrooms, y=log_price)) +
  geom_point(colour = "orange", size = 1.5) +
  geom_smooth(method='lm', color='red') +
  ylab("price")

# price vs accommodates
p2 <- ggplot(data_train, aes(x=log_acc, y=log_price)) +
  geom_point(colour = "orange", size = 1.5) +
  geom_smooth(method='lm', color='red') +
  ylab("price")

# price vs bedrooms
p3 <- ggplot(data_train, aes(x=bedrooms, y=log_price)) +
  geom_point(colour = "orange", size = 1.5) +
  geom_smooth(method='lm', color='red') +
  ylab("price")

# price vs room_type
p4 <- ggplot(data_train, aes(x=room_type, y=log_price)) +
  geom_point(colour = "orange", size = 1.5) +
  geom_smooth(method='lm', color='red') +
  ylab("price")

#price vs host_is_superhost
p5 <- ggplot(data_train, aes(x=host_is_superhost, y=log_price)) +
  geom_point(colour = "orange", size = 1.5) +
  geom_smooth(method='lm', color='red') +
  ylab("price")

# price vs cancellation policy
p6 <- ggplot(data_train, aes(x=cancellation_policy, y=log_price)) +
  geom_point(colour = "orange", size = 1.5) +
  geom_smooth(method='lm', color='red') +
  ylab("price")

# price vs cleaning fee
p7 <- ggplot(data_train, aes(x=cleaning_fee, y=log_price)) +
  geom_point(colour = "orange", size = 1.5) +
  geom_smooth(method='lm', color='red') +
  ylab("price")

#price vs extra people
p8 <- ggplot(data_train, aes(x=extra_people, y=log_price)) +
  geom_point(colour = "orange", size = 1.5) +
  geom_smooth(method='lm', color='red') +
  ylab("price")

#price vs security deposit
p9 <- ggplot(data_train, aes(x=security_deposit, y=log_price)) +
  geom_point(colour = "orange", size = 1.5) +
  geom_smooth(method='lm', color='red') +
  ylab("price")

#price vs availability 365
p10 <- ggplot(data_train, aes(x=availability_365, y=log_price)) +
  geom_point(colour = "orange", size = 1.5) +
  geom_smooth(method='lm', color='red') +
  ylab("price")

#price vs instant_bookable
p11 <- ggplot(data_train, aes(x=instant_bookable, y=log_price)) +
  geom_point(colour = "orange", size = 1.5) +
  geom_smooth(method='lm', color='red') +
  ylab("price")

# combining all plots
grid.arrange(p1, p2, p3, p4,p5,p6, p7,p8, p9, p10, p11, ncol = 4)
```

5. Preparing test and train data 
```{r}
library(caTools)
set.seed(1)
data_train <- data_train %>% drop_na(log_price) %>% drop_na(log_acc) %>% drop_na(bedrooms) %>% drop_na(bathrooms) %>% drop_na(room_type) %>% drop_na(host_is_superhost) %>% drop_na(cancellation_policy) %>% drop_na(availability_365) %>% drop_na(cleaning_fee) %>% drop_na(extra_people) %>% drop_na(security_deposit) %>% drop_na(instant_bookable) %>% drop_na(maximum_nights) %>% drop_na(minimum_nights) %>% drop_na(property_type) %>% drop_na(bed_type) %>% drop_na(latitude) %>% drop_na(longitude) %>% drop_na(number_of_reviews) %>% drop_na(availability_365)

sample_size <- floor(0.75 * nrow(data_train))

set.seed(1)
train_ind <- sample(seq_len(nrow(data_train)), size = sample_size)
train <- data_train[train_ind,]
test <- data_train[-train_ind,]
```


6. Predicting Price using Linear Regression Model
Using log transformed Price and Accommodates - LM_model3
```{r predict-price}
set.seed(100)
#trainingRowIndex <- sample(1:nrow(data_train), 0.8 *nrow(data_train))
#trainingData <- data_train[trainingRowIndex,]
#testData <- data_train[-trainingRowIndex,]

lm_model <- lm(formula = log_price ~ room_type + log_acc + bedrooms + bathrooms + property_type + host_is_superhost + cleaning_fee + cancellation_policy + extra_people + security_deposit + instant_bookable + maximum_nights + minimum_nights + bed_type + latitude + longitude + number_of_reviews + availability_365, data = train)

price_pred <- predict(lm_model, test)

summary(lm_model)

LM_pred <- data.frame(cbind(Actual = test$log_price, Predicted= price_pred))
correlation_accuracy <- cor(LM_pred)

head(LM_pred)
correlation_accuracy
plot(lm_model)

#ggplot(data = testData, aes(x = actual_pred$Actual, y = actual_pred$Predicted)) +
#geom_point() +
#stat_smooth(method = "lm", col = "dodgerblue3") +
#theme(panel.background = element_rect(fill = "white"),
#axis.line.x=element_line(),
#axis.line.y=element_line()) +
#ggtitle("Linear Model Fitted to Data") + ylab("Predicted") + xlab("Actuals")
```
Update: R-squared value is 69.82%.

7. Correlation matrix
```{r corr}
data.corr <- data_train %>% filter(!is.na(log_acc)) %>% filter(!is.na(bathrooms)) %>% filter(!is.na(bedrooms)) %>% filter(!is.na(availability_365)) %>% filter(!is.na(cleaning_fee)) %>% filter(!is.na(extra_people)) %>% filter(!is.na(security_deposit)) %>% filter(!is.na(maximum_nights)) %>% filter(!is.na(minimum_nights)) %>% filter(!is.na(latitude)) %>% filter(!is.na(longitude)) %>% filter(!is.na(number_of_reviews)) %>%
  select(log_price, price, bedrooms, bathrooms, log_acc, availability_365, cleaning_fee, security_deposit, maximum_nights, minimum_nights, extra_people, latitude, longitude, number_of_reviews)
kable(cor(data.corr)) %>% kable_styling()
```
There is some correlation between the variables, but not strong enough to worry about multicollinearity.

#### Decision Trees
1. Create a CART model - with log price and bedroom
```{r Cart1}
# Implement CART model
library(rpart)
library(rpart.plot)
CARTmodel1 = rpart(log_price ~ bedrooms, data = train, cp =0.001)
prp(CARTmodel1)
```

2. Making predictions
```{r pred1}
# Make predictions
predDT = predict(CARTmodel1, newdata = test)

#SSE
SSE = sum((predDT - test$log_price)^2)
cat("SSE = ",SSE)
# RMSE
RMSE = sqrt(mean((predDT - test$log_price)^2))
cat("\nRMSE = ",RMSE)

# Baseline
baseline = mean(train$log_price)
cat("\nbaseline = ",baseline)

# SSE of baseline model on testing set
SSEb = sum((baseline - test$log_price)^2)
cat("\nSSEb = ", SSEb)

# R^2
Rsquared = 1 - SSE/SSEb
cat("\nR squared = ",Rsquared)

#Actual vs Predicted
DT_pred <- data.frame(cbind(Actual = test$log_price, Predicted = predDT))
head(DT_pred)

```

3. Creating CART model to predict log(price) using variables: log(acc), bedrooms, bathrooms, room_type, host_is_superhost, cancellation_policy, cleaning_fee, availability_365, 

```{r}
CARTmodel2 = rpart(log_price ~ room_type + log_acc + bedrooms + bathrooms + property_type + host_is_superhost + cleaning_fee + cancellation_policy + extra_people + security_deposit + instant_bookable + maximum_nights + minimum_nights + bed_type + latitude + longitude + number_of_reviews, data = train, cp =0.001)
prp(CARTmodel2)
```

4. Make predictions:
```{r}
# Make predictions
predDT2 = predict(CARTmodel2, newdata = test)

#head(predTest)

#SSE
SSE = sum((predDT2 - test$log_price)^2)
cat("SSE = ",SSE)
# RMSE
RMSE = sqrt(mean((predDT2 - test$log_price)^2))
cat("\nRMSE = ",RMSE)

# Baseline
baseline = mean(train$log_price)
cat("\nbaseline = ",baseline)

# SSE of baseline model on testing set
SSEb = sum((baseline - test$log_price)^2)
cat("\nSSEb = ", SSEb)

# R^2
Rsquared = 1 - SSE/SSEb
cat("\nR squared = ",Rsquared)

#Actual vs Predicted
DT_pred2 <- data.frame(cbind(Actual = test$log_price, Predicted = predDT2))
head(DT_pred2)
plot(DT_pred2)

```
Using Decision Trees, R-squared value is 61.69%.

#### Random Forest

1. Building a Random Forest

mtry = # of variables: ranges from 1 to p (# of predictor variables)
mtry = p -> bagging
mtry = 1 -> split variable is completely random
sampsize = the number of samples to train on. The default value is 63.25% of the training set since this is the expected value of unique observations in the bootstrap sample. Lower sample sizes can reduce the training time but may introduce more bias than necessary. Increasing the sample size can increase performance but at the risk of overfitting because it introduces more variance. Typically, when tuning this parameter we stay near the 60-80% range.

```{r RTF}
library(randomForest)
library(party)
library(inTrees)
library(randomForestExplainer)
library(sandwich)
set.seed(1)

RFmodel1 <- randomForest(formula = log_price ~room_type + log_acc + bedrooms + bathrooms + property_type + host_is_superhost + cleaning_fee + cancellation_policy + extra_people + security_deposit + instant_bookable + maximum_nights + minimum_nights + bed_type + latitude + longitude + number_of_reviews + availability_365, data=train, nodesize = 100, ntree = 1000, mtry = 8, importance = TRUE, na.action = na.omit)

RFmodel1
#getTree(RFmodel1, 1)
plot(RFmodel1)

round(importance(RFmodel1),2)
```

2. Intepreting Model
```{r RTF-results}
explain_forest(RFmodel1, interactions = TRUE, data = train)

#Variable Importance Measure
importance_frame <- measure_importance(RFmodel1)
save(importance_frame, file = "importance_frame.rda")
load("importance_frame.rda")
importance_frame

#Multi Way Importance Plot
plot_multi_way_importance(RFmodel1, size_measure = "no_of_nodes") # gives the same result as below but takes longer
plot_multi_way_importance(importance_frame, size_measure = "no_of_nodes")

#Comparing Measures Using ggpairs
plot_importance_ggpairs(RFmodel1) 
plot_importance_ggpairs(importance_frame)
```

3. Using Random Forest to make predictions
```{r predict}
# Make predictions
predTest <- predict(RFmodel1, newdata = test, type = "response")

RT_pred <- data.frame(cbind(Actual = test$log_price, Predicted= predTest))

RT_pred
```

4. Estimating Performance for Random Forest 
````{r}
# SSE
SSE = sum((predTest - test$log_price)^2)
cat("SSE = ",SSE)

# RMSE
RMSE = sqrt(mean((predTest - test$log_price)^2))
cat("\nRMSE = ",RMSE)

# Baseline
baseline = mean(test$log_price)
cat("\nBaseline = ",baseline)

# SSE of baseline model on testing set
SSEb = sum((baseline - test$log_price)^2)
cat("\nSSEb = ",SSEb)

# R^2
Rsquared = 1 - SSE/SSEb
cat("\nRsquared = ",Rsquared)
```
Random Tree Forest model gives an R-squared value of 66.7%.

5. Parameter Tuning - Grid Search
https://uc-r.github.io/random_forests
```{r}
library(mlbench)
library(caret)
library(e1071)

set.seed(1)

trControl <- trainControl(method = "cv", number = 10, search = "grid", savePredictions = TRUE)
rf_default <- train(log_price ~room_type + log_acc + bedrooms + bathrooms + property_type + host_is_superhost + cleaning_fee + cancellation_policy + extra_people + security_deposit + instant_bookable + maximum_nights + minimum_nights + bed_type + latitude + longitude + number_of_reviews, data = train, method = "rf", metric = "Rsquared", trControl = trControl)
print(rf_default)

```

6. Building a new Random Forest with parameter - mtry = 17
```{r RTF2}
library(randomForest)
library(party)
library(inTrees)
library(randomForestExplainer)
set.seed(1)

RFmodel2 <- randomForest(formula = log_price ~room_type + log_acc + bedrooms + bathrooms + property_type + host_is_superhost + cleaning_fee + cancellation_policy + extra_people + security_deposit + instant_bookable + maximum_nights + minimum_nights + bed_type + latitude + longitude + number_of_reviews +availability_365, data=train, nodesize = 100, ntree = 1000, mtry = 17, importance = TRUE, na.action = na.omit)

RFmodel2
#getTree(RFmodel1, 1)
plot(RFmodel2)

round(importance(RFmodel2),2)
```

7. Intepreting New Model
```{r RTF-results}
explain_forest(RFmodel2, interactions = TRUE, data = train)

#Variable Importance Measure
importance_frame <- measure_importance(RFmodel2)
save(importance_frame, file = "importance_frame.rda")
load("importance_frame.rda")
importance_frame

#Multi Way Importance Plot
plot_multi_way_importance(RFmodel2, size_measure = "no_of_nodes") # gives the same result as below but takes longer
plot_multi_way_importance(importance_frame, size_measure = "no_of_nodes")

#Comparing Measures Using ggpairs
plot_importance_ggpairs(RFmodel2) 
plot_importance_ggpairs(importance_frame)
```

8. Using new Random Forest to make predictions
```{r predict}
# Make predictions
predTest2 <- predict(RFmodel2, newdata = test, type = "response")

RT_pred2 <- data.frame(cbind(Actual = test$log_price, Predicted= predTest2))

RT_pred2
```

9. Parameters for Random Forest 
````{r}
# SSE
new_SSE = sum((predTest2 - test$log_price)^2)
cat("SSE = ",new_SSE)

# RMSE
new_RMSE = sqrt(mean((predTest2 - test$log_price)^2))
cat("\nRMSE = ",new_RMSE)

# Baseline
new_baseline = mean(test$log_price)
cat("\nBaseline = ",new_baseline)

# SSE of baseline model on testing set
new_SSEb = sum((new_baseline - test$log_price)^2)
cat("\nSSEb = ",new_SSEb)

# R^2
new_Rsquared = 1 - (new_SSE/new_SSEb)
cat("\nRsquared = ",new_Rsquared)
```
Random Tree Forest model gives an R-squared value of 66.94%.

10. Measuring Predictive Accuracy
```{r}
# create training and validation data 
library(rsample)
set.seed(123)
valid_split <- initial_split(train, .8)

# training data
ames_train_v2 <- analysis(valid_split)

# validation data
ames_valid <- assessment(valid_split)
x_test <- ames_valid[setdiff(names(ames_valid), c("log_acc","bedrooms", "bathrooms","property_type","host_is_superhost", "cleaning_fee", "cancellation_policy", "extra_people", "security_deposit","instant_bookable","maximum_nights","minimum_nights","bed_type","latitude","longitude","number_of_reviews"))]
y_test <- ames_valid$log_price

rf_oob_comp <- randomForest(
  formula = log_price ~room_type + log_acc + bedrooms + bathrooms + property_type + host_is_superhost + cleaning_fee + cancellation_policy + extra_people + security_deposit + instant_bookable + maximum_nights + minimum_nights + bed_type + latitude + longitude + number_of_reviews,
  data    = ames_train_v2,
  xtest   = x_test,
  ytest   = y_test
)

# extract OOB & validation errors
oob <- sqrt(rf_oob_comp$mse)
validation <- sqrt(rf_oob_comp$test$mse)

# compare error rates
tibble::tibble(
  `Out of Bag Error` = oob,
  `Test error` = validation,
  ntrees = 1:rf_oob_comp$ntree
) %>%
  gather(Metric, RMSE, -ntrees) %>%
  ggplot(aes(ntrees, RMSE, color = Metric)) +
  geom_line() +
  scale_y_continuous(labels = scales::dollar) +
  xlab("Number of trees")
```


### ANOVA - to compare categorical data
```{r anova}
AOV_res <- summary(aov(log_price ~ factor(property_type) + factor(neighbourhood_cleansed), data = data_train))
AOV_res
```
Results:
As we can see, property type and neighborhood do have a significant impact on price since the p-value is much smaller than the significance level.

### GLM
1. Fitting model
```{r glm}
library(jtools)
library(kableExtra)

glm_1 <- glm(log_price ~ room_type + log_acc + bedrooms + bathrooms + property_type + host_is_superhost + cleaning_fee + cancellation_policy + extra_people + security_deposit + instant_bookable + maximum_nights + minimum_nights + bed_type + latitude + longitude + number_of_reviews + availability_365, data = data_train, family = gaussian(link  = "log"))
summary(glm_1)

## review the model summary
summary(glm_1)
hist(glm_1$fitted.values)

## check out fitted values against the originals
plot(data_train$log_price, glm_1$fitted.values)
abline(a=0, b=1)

plot(data_train$log_price, glm_1$fitted.values, xlim = c(0, 37000), ylim = c(0, 37000))
abline(a=0, b=1)

summ(glm_1, exp = TRUE)

## MSE and RMSE
mean((glm_1$fitted.values - data_train$log_price)^2)
sqrt(mean((glm_1$fitted.values - data_train$log_price)^2))
```
Results:
With transforming price on the log scale, the following features are found to be significant (same as previous model):
- Private Room Type
- Shared Room Type
- Accommodates
- Bedrooms
- Bathrooms
- Property Type
- Host is Superhost
- Availability 365
- Cleaning fee
- Security Deposit
- Instant Bookable
- Minimum Nights
- Latitude
- Longitude
- Number of Reviews
- Availability 365

The R-squared metric is 83%.s

2. Price Prediction with GLM
```{r glm_predict}
set.seed(100)

glm_model <- glm(log_price ~ room_type + log_acc + bedrooms + bathrooms + property_type + host_is_superhost + cleaning_fee + cancellation_policy + extra_people + security_deposit + instant_bookable + maximum_nights + minimum_nights + bed_type + latitude + longitude + number_of_reviews + availability_365, family = gaussian(link  = "log"), data = train)

price_pred2 <- predict(glm_model, test)

#summary(glm_model)
summ(glm_model, exp = TRUE)

GLM_pred <- data.frame(cbind(Actual = test$log_price, Predicted= price_pred2))
correlation_accuracy <- cor(GLM_pred)

head(GLM_pred)
correlation_accuracy
plot(glm_model)


```
Using the mcFadden Pseudo R-squared parameter, we get an R-squared value of 68%.

## Model Evaluation
```{r}
hist(LM_pred)
hist(RT_pred)

```

Model is evaluated based on R-squared value i.e. % of variablility explained by the model.
Predicting price on:
1. Linear Regression has an R-squared value of 69.82%.
2. Decision Trees have an R-squared value of  61.69%
3. Random Tree Forest has an R-square value of 66.94%.
4. GLM has an R-squared value of 68%.

Based on this, Linear Regression wins.

## Utility Function
1. For Linear Regression
```{r util-LR}
LM_pred$diff <- LM_pred$Actual - LM_pred$Predicted
diff <- sum(LM_pred$diff)
head(LM_pred)
cat("Sum of diff = ", diff)
```
A loss of $6.6 is predicted with linear regression.

2. Decision Trees
```{r util-DT}

DT_pred2$diff <- DT_pred2$Actual - DT_pred2$Predicted
diff2 <- sum(DT_pred2$diff)
head(DT_pred2)
cat("/n Diff = ", diff2)
```


3. Random Tree Forest
```{r util-RTF}
RT_pred2$diff <- RT_pred2$Actual - RT_pred2$Predicted
diff2 <- sum(RT_pred2$diff)

head(RT_pred2)
cat("\n Diff = ",diff2)
```
A loss of $7.5 predicted

4. GLM
```{r util-GLM}
GLM_pred$diff <- GLM_pred$Actual - GLM_pred$Predicted
diff <- sum(GLM_pred$diff)
head(GLM_pred)
cat("Sum of diff = ", diff)
```

Linear Regression wins.
